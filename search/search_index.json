{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#welcome-to-qihangs-log","title":"Welcome to Qihang\u2019s Log \ud83d\udc4b","text":"<p>Hi, this is Qihang\ud83d\udc4b. </p> <p>I\u2019m documenting my learning process on this blog, covering everything related to Machine Learning.</p> css add shadow <p> </p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html","title":"Why the Exponential? From Max\u2011Entropy RL to the Boltzmann Distribution","text":"<p>Modern RL, attention mechanisms, classification, energy-based modeling, and statistical mechanics keep arriving at the same exponential shape:</p> \\[ p(x)\\;\\propto\\;\\exp(\\text{logits or reward}(x)/T)\\quad\\text{or}\\quad p(x)\\;\\propto\\;\\exp(-E(x)/T). \\] <p>Why does the exponential keep showing up, and what does the \"temperature\" actually do?</p> <p>To demonstrate some intuitions, this post will go through three views to explain why the exponential form is ubiquitous:</p> <p>1) Maximum Entropy Reinforcement Learning (MaxEnt RL): the optimal soft policy is an exponential of reward with an entropy-weighted objective;</p> <p>2) Energy-based Models (EBMs): probabilities are Boltzmann weights with a partition function \\(Z\\);</p> <p>3) Statistical Mechanics: Jaynes's maximum-entropy principle and ensemble theory (Boltzmann \u2192 Gibbs \u2192 Shannon \u2192 Jaynes) make the exponential inevitable under mean-value constraints.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Why the Exponential? From Max\u2011Entropy RL to the Boltzmann Distribution<ul> <li>Table of Contents</li> <li>Maximum Entropy Reinforcement Learning<ul> <li>Optimal-Soft Policy Form</li> <li>Proof of Optimal-Soft Form<ul> <li>\\(J(\\pi,T)\\) is a Concave function</li> <li>Get The Optimal-Soft Policy Via Method of Lagrange Multipliers</li> </ul> </li> </ul> </li> <li>Energy-Based Models (EBM)<ul> <li>The Very First Energy-Based Model<ul> <li>Hopfield Network</li> <li>Boltzmann Machine</li> </ul> </li> </ul> </li> <li>Boltzmann Distribution and Gibbs Distribution<ul> <li>The Origin of Boltzmann and Gibbs Distribution<ul> <li>Gibbs Distribution: A More General Framework</li> </ul> </li> </ul> </li> <li>Jaynes's Principle of Maximum Entropy<ul> <li>History</li> </ul> </li> <li>Conclusion</li> </ul> </li> </ul>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#maximum-entropy-reinforcement-learning","title":"Maximum Entropy Reinforcement Learning","text":"<p>Last week I was reading the paper All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning<sup>1</sup>. In their proof to prove that DPO is equivalent to first train a reward model and then do on-policy RL to optimize the LLM with the help of the reward model, they used the conclusion from Maximum Entropy Reinforcement Learning (MaxEnt RL).</p> <p>Since the MaxEnt RL framework underlies many modern reinforcement learning algorithms, I revisit its derivation here. During this process, I also found its close connection to statistical mechanics, which I will elaborate on below.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#optimal-soft-policy-form","title":"Optimal-Soft Policy Form","text":"<p>In LLM post-training, when we have a reward function \\(r_{\\theta}(\\xi, s_0)\\) to calculate the reward of a given trajectory \\(\\xi\\) generated by a LLM \\(\\pi \\in \\Pi\\) conditioned on the initial state \\(s_0\\), i.e. \\(\\xi \\sim \\pi(\\cdot \\mid s_0)\\), we want to find the optimal-soft policy \\(\\pi_{r_{\\theta}, T}^\\star\\) that maximizes the expected reward while also not too complex so that it won't overfit the learned reward model, which itself acts as a synthetic data proxy.</p> <p>Furthermore, we include an entropy regularization term, following Jaynes's maximum entropy principle, to encourage exploration and prevent degenerate deterministic policies.</p> <p>The Optimal-Soft Policy Form is defined as follows:</p> \\[ \\begin{align}   \\pi _{r _{\\theta}, T}^\\star    &amp;= \\operatorname*{argmax} _{\\pi\\in\\Pi} \\;   \\mathbb{E} _{\\xi\\sim\\pi} \\! \\left[r _{\\theta}(\\xi, s_0)\\right] + T \\cdot \\mathcal{H}(\\pi)\\\\   \\iff &amp; \\pi _{r _{\\theta}, T}^\\star(\\xi \\mid s_0)   = \\frac{\\exp\\!\\big(r _{\\theta}(\\xi, s_0)/T\\big)}{Z(r _{\\theta},s_0,T)},\\\\   &amp; \\text{where }Z(r _{\\theta},s_0,T) = \\sum _{\\xi'\\in \\Xi}\\exp\\!\\big(r _{\\theta}(\\xi', s_0)/T\\big) \\end{align} \\] <p>Where:</p> <ul> <li>\\(\\pi\\) is a policy in \\(\\Pi\\), \\(\\Pi\\) is the set of all possible policies;</li> <li>Both \\(\\xi\\) and \\(\\xi'\\) are trajectories in \\(\\Xi\\), \\(\\Xi\\) is the set of all possible trajectories;</li> <li>\\(s_0\\) is the initial state, \\(s_0 \\sim \\rho(s_0)\\), it is corresponding to the prompt in LLM post-training;</li> <li>\\(T\\) is a temperature parameter that controls the exploration-exploitation trade-off in Reinforcement Learning, or can be explained as a regularization strength that controls the importance of the entropy term in the objective function.</li> </ul>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#proof-of-optimal-soft-form","title":"Proof of Optimal-Soft Form","text":"<p>If we denote \\(\\pi_i = \\pi_{r_{\\theta}}(\\xi_i \\mid s_0), \\; r_i = r_{\\theta}(\\xi_i \\mid s_0)\\):</p> \\[ \\begin{aligned} J(\\pi,T) &amp;=\\mathbb{E} _{\\xi\\sim\\pi} \\! \\left[r _{\\theta}(\\xi, s_0)\\right] + T \\cdot \\mathcal{H}(\\pi) \\\\ &amp;= \\sum_i \\pi_i r_i -T \\sum_i \\pi_i \\log \\pi_i\\\\ \\end{aligned} \\] \\[ J(\\pi,T) = \\underbrace{\\sum_i \\pi_i r_i}_{R(\\pi)\\text{: Expected Reward}} + T \\cdot \\underbrace{(- \\sum_i \\pi_i \\log \\pi_i)}_{H(\\pi)\\text{: Shannon Entropy}} \\] <p>In this formulation, the variables are \\(\\{\\pi_1,...,\\pi_{\\mid \\Xi \\mid}\\}\\), the constraint is \\(\\displaystyle\\sum_{i = 1}^{\\mid \\Xi \\mid} \\pi_i = 1\\).</p> <p></p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#jpit-is-a-concave-function","title":"\\(J(\\pi,T)\\) is a Concave function","text":"<p>Given: + \\(R(\\pi) = \\displaystyle\\sum_{i = 1}^{\\mid \\Xi \\mid} \\pi_i r_i\\) is a linear function with respect to variables \\(\\{p_1,...,p_{\\mid \\Xi \\mid}\\}\\). And thus it is a concave function. + \\(H(\\pi) = - \\displaystyle\\sum_{i = 1}^{\\mid \\Xi \\mid} \\pi_i \\log \\pi_i\\) is a strictly concave function. When \\(T &gt; 0\\), \\(T \\cdot H(\\pi)\\) is also strictly concave.</p> <p>Thus when \\(T &gt; 0\\), the sum \\(J(\\pi,T) = R(\\pi) + T \\cdot H(\\pi)\\) is strictly concave (since the sum of a concave and a strictly concave function is strictly concave).</p> <p>Because \\(J(\\pi,T)\\) is strictly concave under the normalization constraint when \\(T &gt; 0\\), the stationary point found below is the unique global optimum.</p> <p></p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#get-the-optimal-soft-policy-via-method-of-lagrange-multipliers","title":"Get The Optimal-Soft Policy Via Method of Lagrange Multipliers","text":"<p>Given \\(J(\\pi,T)\\) is a Concave function, we have the following Lagrange Function:</p> \\[ \\begin{align} \\mathcal{L}(\\pi, T, \\lambda)  &amp;= \\underbrace{J(\\pi,T)}_{\\text{Original Target}} - \\lambda \\cdot \\underbrace{(\\sum_i \\pi_i - 1)}_{\\text{Constrain}} \\\\ &amp;= \\sum_i \\pi_i r_i -T \\sum_i \\pi_i \\log \\pi_i - \\lambda \\left( \\sum_{i = 1}^{\\mid \\Xi \\mid} \\pi_i - 1 \\right) \\end{align} \\] <p>In order to find the maximum, we take the partial derivative with respect to any \\(\\pi_i\\) and set it to zero: $$ \\frac{\\partial \\mathcal{L}}{\\partial \\pi_i} = r_i - T (\\log \\pi_i + 1)  - \\lambda = 0 $$ And we can get  \\(\\pi_i\\):</p> \\[ \\begin{align}     \\log \\pi_i &amp;= -1 -\\lambda/T + r_i/T \\\\     \\pi_i &amp;= \\exp(-1 - \\lambda/T) \\cdot \\exp(r_i/T) \\end{align} \\] <p>Thus: $$ \\pi_{r_{\\theta}}^\\star(\\xi_i \\mid s_0) \\propto \\exp\\big(r_{\\theta}(\\xi_i \\mid s_0) / T\\big) $$</p> <p>And Because \\(\\displaystyle\\sum_{i = 1}^{\\mid \\Xi \\mid} \\pi_{r_{\\theta}} = 1\\), we can get the final form of Optimal-Soft Policy:</p> \\[ \\begin{align} \\pi _{r _{\\theta}, T}^\\star(\\xi \\mid s_0) &amp;= \\frac{\\exp\\!\\big(r _{\\theta}(\\xi, s_0)/T\\big)}{Z(r _{\\theta},s_0,T)}\\\\ \\text{where }Z(r _{\\theta},s_0,T) &amp;= \\sum _{\\xi'\\in \\Xi}\\exp\\!\\big(r _{\\theta}(\\xi', s_0)/T\\big) \\end{align} \\]"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#energy-based-models-ebm","title":"Energy-Based Models (EBM)","text":"<p>We have already known that the Optimal-Soft Policy is \\(\\pi_{r_{\\theta}, T}^\\star(\\xi \\mid s_0) = \\frac{\\exp(r _{\\theta}(\\xi, s_0)/T)}{Z(r _{\\theta},s_0,T)}\\), and we can regard it as a special energy-based model. </p> <p>In the context of energy-based models, we call \\(E(\\xi) = -r_{\\theta}(\\xi, s_0)\\) the energy function, while \\(Z(r_{\\theta}, s_0, T)\\) serves as the partition function that normalizes the probabilities.</p> <p>This identification reveals that the softmax over rewards in RL is mathematically identical to the Boltzmann distribution in physics, with reward playing the role of negative energy.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#the-very-first-energy-based-model","title":"The Very First Energy-Based Model","text":""},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#hopfield-network","title":"Hopfield Network","text":"<p>In 1982, J. Hopfield proposed the Hopfield Network <sup>2</sup>, which first introduced the concept of energy function in neural networks. </p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#boltzmann-machine","title":"Boltzmann Machine","text":"<p>The first work that relatively introduced energy based model in Machine Learning should be the paper by Hinton and Sejnowski in 1985 <sup>3</sup>, where they defined the probability distribution of Boltzmann machine via the Boltzmann distribution.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#problem-setting","title":"Problem Setting:","text":"<ul> <li>Given a set of binary vectors (data) \\(\\{v^L\\}\\), where \\(v^L \\in \\{0,1\\}^L\\).</li> <li>In the dataset they used, there are 4 binary vectors with Length \\(8\\): </li> </ul> \\[ \\begin{align}   \\{&amp;(1,0,0,0) -&gt; (1,0,0,0), \\\\     &amp;(0,1,0,0) -&gt; (0,1,0,0), \\\\     &amp;(0,0,1,0) -&gt; (0,0,1,0), \\\\     &amp;(0,0,0,1) -&gt; (0,0,0,1)\\}   \\end{align} \\]"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#the-energy-function-of-boltzmann-machine","title":"The Energy Function of Boltzmann Machine","text":"<p>The energy function of a Boltzmann machine is defined as follows:</p> \\[ E = - \\sum_{i &lt; j} w_{ij} s_i s_j - \\sum_i \\theta_i s_i \\] <p>Where:</p> <ul> <li>\\(E\\) is the energy of a global configuration of the network,</li> <li>\\(w_{ij}\\) are the weights between units \\(i\\) and \\(j\\),</li> <li>\\(s_i\\) is 1 if unit \\(i\\) is on, and 0 if it is off,</li> <li>\\(\\theta_i\\) is a threshold for unit \\(i\\).</li> </ul> <p>As shown in the figure below, in their experiments setting, the Boltzmann machine has totally 10 units: the visible units 1-4 are corresponding to the first 4 digits, the visible units 7-10 are corresponding to the last 4 digits. The hidden units 5-6 are used to construct a information bottleneck and also disentangle the two part of data.</p> <pre><code>flowchart TD\n  subgraph V1 [Visible Units]\n    direction LR\n    v1_1[Unit 1]\n    v1_2[Unit 2]\n    v1_3[Unit 3]\n    v1_4[Unit 4]\n\n    %% use invisible links to force horizontal layout\n    v1_1 ~~~ v1_2 ~~~ v1_3 ~~~ v1_4\n  end\n\n  subgraph H [Hidden Units]\n    direction LR\n    h1[Unit 5]\n    h2[Unit 6]\n\n    %% use invisible links to force horizontal layout\n    h1 ~~~ h2\n  end\n\n  subgraph V2 [Visible Units]\n    direction LR\n    v2_1[Unit 7]\n    v2_2[Unit 8]\n    v2_3[Unit 9]\n    v2_4[Unit 10]\n\n    %% use invisible links to force horizontal layout\n    v2_1 ~~~ v2_2 ~~~ v2_3 ~~~ v2_4\n  end\n\n  V1 ~~~ H\n  H ~~~ V2\n\n  style V1 fill:#e6f3ff,stroke:#333,stroke-width:2px\n  style H fill:#fffbe6,stroke:#333,stroke-width:2px\n  style V2 fill:#e6ffed,stroke:#333,stroke-width:2px</code></pre>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#the-probability-distribution-of-boltzmann-machine","title":"The Probability Distribution of Boltzmann Machine","text":"<p>The probability distribution over the states of the Boltzmann machine is given by the Boltzmann distribution:</p> \\[ \\frac{P_{\\alpha}}{P_{\\beta}} = e^{-(E_{\\alpha} - E_{\\beta})/T} \\] <p>Where:</p> <ul> <li>\\(P_{\\alpha}\\) is the probability of the network being in \\(\\alpha^{th}\\) state;</li> <li>\\(E_{\\alpha}\\) is the energy of the corresponding state.</li> </ul> <p>Thus for arbitrary state \\(\\alpha\\), we can get the probability of the network being in this state:</p> \\[ P_{\\alpha} = \\frac{e^{-E_{\\alpha}/T}}{\\displaystyle\\sum_{\\alpha^{\\prime} \\in \\mathcal{A}} e^{-E_{\\alpha^{\\prime}}/T}} = \\frac{e^{-E_{\\alpha}/T}}{Z} \\] <p>Where:</p> <ul> <li>\\(\\mathcal{A}\\) is the set of all possible states of the network;</li> <li>\\(Z = \\displaystyle\\sum_{\\alpha^{\\prime} \\in \\mathcal{A}} e^{-E_{\\alpha^{\\prime}}/T}\\) is the partition function, ensuring that the probabilities sum to 1.</li> </ul>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#optimization-of-boltzmann-machine","title":"Optimization of Boltzmann Machine","text":"<p>Optimization of Boltzmann machine is to minimize the Kullback-Leibler divergence between the data distribution and the model distribution, which is equivalent to maximizing the log-likelihood of the data under the model.</p> \\[ G = \\sum_{\\alpha} P(V_{\\alpha}) \\log \\frac{P(V_{\\alpha})}{P^{\\prime}(V_{\\alpha})} \\] <p>Where:</p> <ul> <li>\\(P(V_{\\alpha})\\) is the data distribution;</li> <li>\\(P^{\\prime}(V_{\\alpha})\\) is the model distribution defined by energy based model.</li> </ul> <p>They optimize \\(G\\) via stochastic gradient descent on the energy parameters, a precursor to the contrastive divergence training used in later works.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#boltzmann-distribution-and-gibbs-distribution","title":"Boltzmann Distribution and Gibbs Distribution","text":"<p>After the above sections, we can realize that the ubiquitous exponential form in probabilistic models originates from statistical mechanics:</p> <ul> <li> <p>Generally speaking, the Boltzmann distribution refers specifically to the case under the canonical ensemble (NVT, i.e., constant particle number, volume, and temperature).</p> </li> <li> <p>The Gibbs distribution/measure is a more general concept that refers to any probability distribution of the form \\(p \\propto e^{-\\beta H}\\) and can be applied to other physical ensembles.</p> </li> </ul> <p>The canonical definition of the Boltzmann distribution is as follows:</p> \\[ \\displaystyle P(x) = \\frac{e^{-\\beta E(x)}}{Z} \\] <p>where:</p> <ul> <li>\\(\\displaystyle x\\) represents a specific microstate of the system.</li> <li>\\(\\displaystyle E(x)\\) is the energy of the system in the microstate \\(x\\).</li> <li>\\(\\displaystyle \\beta = \\frac{1}{k_B T}\\) is the inverse temperature. \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the thermodynamic temperature.</li> <li>\\(\\displaystyle Z = \\sum_{x'} e^{-\\beta E(x')}\\) is the partition function, a normalization constant ensuring that the sum of probabilities over all states is 1.</li> </ul>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#the-origin-of-boltzmann-and-gibbs-distribution","title":"The Origin of Boltzmann and Gibbs Distribution","text":"<p>This beautiful exponential form does not arise out of thin air. </p> <p>Based on the equal probability assumption of the microcanonical ensemble, weak coupling between the system and the heat reservoir, and applying Jaynes's maximum entropy principle (or equivalent combinatorial counting methods) under the constraint of a given average energy, we can derive the Boltzmann distribution of the canonical ensemble. </p> <p>It is consistent with the macroscopic evolutionary direction described by the second law of thermodynamics, but more directly speaking, it is the result of equilibrium statistics and maximum entropy inference.</p> <p>Let's break down the derivation process:</p> <ol> <li> <p>Entropy defined by Boltzmann under Microcanonical Ensemble: </p> <p>For an isolated system with fixed energy, Boltzmann's hypothesis states that all accessible microstates are equally probable. The entropy is defined by the famous Boltzmann formula:</p> \\[ S = k_B \\ln W \\] <p>where \\(W\\) is the total number of microstates.</p> <p></p> </li> <li> <p>Entropy defined by Gibbs under Canonical Ensemble: </p> <p>When the system is no longer isolated but is in contact with a large heat reservoir at constant temperature, the energy of the system will fluctuate. At this point, the probabilities of different energy microstates are no longer equal. </p> <p>Gibbs generalized Boltzmann's entropy definition to the canonical ensemble, defining the entropy as:</p> \\[ S = -k_B \\sum_{x \\in \\mathcal{X}} p(x) \\ln p(x) \\] <p>where: - \\(p(x)\\) is the probability of the system being in microstate \\(x\\).  - \\(\\mathcal{X}\\) is the set of all possible microstates of the system.</p> <p>What's worth noting is that this is exactly the Shannon entropy in information theory when \\(k_B\\) is set to 1. This equivalence between physical and informational entropy later became the foundation of Jaynes's reinterpretation.</p> <p></p> </li> <li> <p>Calculate the explicit form of the distribution of whole system:</p> <ul> <li> <p>From the perspective of maximizing Gibbs Entropy:</p> <p>In order to explicitly calculate the probability distribution \\(p(x)\\), following Jaynes's principle of maximum entropy, we maximize the Gibbs entropy:</p> \\[ S = -k_B \\sum_{x \\in \\mathcal{X}} p(x) \\ln p(x). \\] <p>Under the constraint of fixed average energy of the system, i.e \\(\\displaystyle\\mathbb{E}_{x \\sim p(x)} [E(x)] = U\\), and the normalization condition \\(\\displaystyle\\sum_x p(x) = 1\\), we can formulate the constrained optimization problem as follows:</p> \\[ \\begin{align} \\max_{p(x)} \\quad &amp; -k_B \\sum_{x \\in \\mathcal{X}} p(x) \\ln p(x) \\\\ \\text{subject to} \\quad &amp; \\sum_{x \\in \\mathcal{X}} p(x) = 1, \\\\ &amp; \\sum_{x \\in \\mathcal{X}} p(x) E(x) = U. \\end{align} \\] <p>With the method of lagrange multipliers mentioned before, the unique solution to this constrained optimization problem is the Boltzmann distribution:</p> \\[ p(x) = \\frac{e^{-\\beta E(x)}}{Z}  \\] <p>where:</p> <ul> <li>\\(\\beta = \\frac{1}{k_B T}\\) is the inverse temperature, </li> <li>\\(Z = \\displaystyle \\sum_{x' \\in \\mathcal{X}} e^{-\\beta E(x')}\\) is the partition function.</li> </ul> </li> <li> <p>From the perspective of minimizing Helmholtz free energy:</p> <p>The Helmholtz free energy \\(F\\) (for canonical ensemble at constant \\(N\\), \\(V\\), \\(T\\)) is defined as:</p> \\[ \\begin{align} F &amp;= U - TS \\\\   &amp;= \\sum_{x \\in \\mathcal{X}} p(x) E(x) + k_B T \\sum_{x \\in \\mathcal{X}} p(x) \\ln p(x) \\end{align} \\] <p>where:</p> <ul> <li>\\(U\\) is the internal energy, </li> <li>\\(T\\) is the temperature,</li> <li>\\(S\\) is the entropy.</li> </ul> <p>We can see that:</p> <p>At equilibrium, minimizing the Helmholtz free energy \\(F\\) with respect to the probability distribution \\(p(x)\\) is equivalent to maximizing the entropy \\(S\\) subject to the constraint of fixed average energy \\(\\langle E \\rangle = U\\). Thus, both formulations\u2014entropy maximization under energy constraint and free-energy minimization\u2014lead to the same exponential family distribution.</p> </li> </ul> <p></p> </li> </ol> <p>This distribution relies on several key assumptions, including: weak coupling between the system and the heat reservoir, the thermodynamic limit (large number of degrees of freedom), and exhibiting ergodicity or typicality.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#gibbs-distribution-a-more-general-framework","title":"Gibbs Distribution: A More General Framework","text":"<p>The concept of the \"Gibbs distribution\" is more general and can unify the description of equilibrium distributions across different statistical ensembles. This is typically achieved by introducing an effective Hamiltonian \\(H_{\\text{eff}}\\), allowing the distribution to be uniformly expressed as \\(p \\propto e^{-\\beta H_{\\text{eff}}}\\).</p> <p>All partition functions share the core form: $$ \\text{Partition Function} = \\sum_{\\text{states}} e^{-\\beta (\\text{Energy-like term})} $$ where \\(\\beta = 1 / (k_B T)\\).</p> Ensemble Constraints Random Variables Effective Hamiltonian \\(H_{\\text{eff}}\\) Partition Function Thermodynamic Free Energy Statistical Free Energy Microcanonical \\(N, V, E\\) (None) Not applicable\u00b9 \\(\\Omega(E,V,N)\\) \\(S=k_B\\ln\\Omega\\) (Fundamental definition) Canonical \\(N, V, T\\) Energy \\(E\\) \\(E\\) \\(Z = \\sum_i e^{-\\beta E_i}\\) \\(F = U - TS\\) \\(F = -k_B T \\ln Z\\) Grand Canonical \\(\\mu, V, T\\) Energy \\(E\\), Particle Number \\(N\\) \\(E-\\mu N\\) \\(\\Xi = \\sum_{N,i} e^{-\\beta(E_i - \\mu N)}\\) \\(\\Omega = U -TS -\\mu N\\) \\(\\Omega = -k_B T \\ln \\Xi\\) Isothermal-Isobaric \\(N, P, T\\) Energy \\(E\\), Volume \\(V\\) \\(E+PV\\) \\(\\Delta = \\int e^{-\\beta(E + PV)} dV\\) \\(G = U - TS + PV\\) \\(G = -k_B T \\ln \\Delta\\) <p>Key Relationships:</p> <ul> <li>From \\(Z\\) to \\(\\Xi\\): \\(\\Xi = \\sum_{N=0}^{\\infty} e^{\\beta \\mu N} Z(N,V,T)\\) (introduces particle number fluctuations)</li> <li>From \\(Z\\) to \\(\\Delta\\): \\(\\Delta = \\int_0^{\\infty} e^{-\\beta PV} Z(N,V,T) dV\\) (introduces volume fluctuations)</li> </ul> <p>Notes:</p> <p>[1] The microcanonical ensemble follows a uniform distribution \\(P(\\text{state}) = 1/\\Omega\\) for accessible states with energy \\(E\\), rather than the exponential Gibbs form \\(p \\propto e^{-\\beta H_{\\text{eff}}}\\). It serves as the foundation for other ensembles but doesn't itself belong to the Gibbs distribution family.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#jayness-principle-of-maximum-entropy","title":"Jaynes's Principle of Maximum Entropy","text":"<p>Jaynes (1957) recast statistical mechanics as inference under incomplete information: among all distributions consistent with the constraints you actually know, choose the one with maximum Shannon entropy. This yields the canonical Boltzmann/Gibbs form and, equivalently, a free\u2011energy minimization principle.</p>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#history","title":"History","text":"<ul> <li> <p>1870s (Boltzmann): Proposed the statistical interpretation of entropy, connecting microscopic configurations to macroscopic thermodynamic quantities through the celebrated formula:   $$   S = k_B \\ln W   $$   where \\(S\\) is entropy, \\(k_B\\) is the Boltzmann constant, and \\(W\\) is the number of accessible microstates. This marks the birth of statistical mechanics and introduces the probabilistic foundation of thermodynamics.</p> </li> <li> <p>1902 (Gibbs): Generalized Boltzmann's discrete-state formulation into a probabilistic ensemble theory. For systems in thermal equilibrium with varying probabilities \\(p_i\\) for each microstate \\(i\\), Gibbs defined:   $$   S = -k_B \\sum_i p_i \\ln p_i   $$   and derived the canonical and grand canonical ensembles, where the equilibrium distribution takes the now-fundamental exponential form:   $$   p_i = \\frac{e^{-\\beta E_i}}{Z}, \\quad Z = \\sum_i e^{-\\beta E_i}.   $$</p> </li> <li> <p>1948 (Shannon): Reinterpreted Gibbs's entropy in the context of communication theory as information entropy: <sup>4</sup>   $$   H = -\\sum_x p(x) \\ln p(x),   $$   formally decoupling the mathematical structure of entropy from its physical meaning and laying the foundation of information theory.</p> </li> <li> <p>1957 (Jaynes): Unified Boltzmann\u2013Gibbs statistical mechanics with Shannon's information theory in \"Information Theory and Statistical Mechanics I/II\" <sup>5</sup>.   Jaynes postulated that thermodynamic equilibrium corresponds to the maximum entropy distribution consistent with known constraints:   $$   \\max_{p(x)} \\; -\\sum_x p(x)\\ln p(x)   $$   subject to \\(\\sum_x p(x)=1\\) and \\(\\sum_x p(x)E(x)=U\\).   Solving this yields the canonical Boltzmann distribution:   $$   p(x)=\\frac{e^{-\\beta E(x)}}{Z}.   $$   This reframes statistical mechanics as inference under incomplete information, where maximizing entropy is equivalent to minimizing free energy.</p> </li> </ul>"},{"location":"2025/10/06/max-ent-rl-and-boltzmann-distribution.html#conclusion","title":"Conclusion","text":"<p>Although this note cannot cover every softmax function in machine learning, it provides intuition by illustrating the cases above, demonstrating why the exponential form appears ubiquitously across reinforcement learning, energy-based models, and statistical mechanics.</p> <p>Whenever we introduce Shannon or Gibbs entropy as a regularization or principled constraint, the Gibbs distribution naturally emerges as the optimal solution\u2014making the exponential form not just mathematically convenient, but fundamentally inevitable.</p> <ol> <li> <p>Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J. Andrew Bagnell. All roads lead to likelihood: the value of reinforcement learning in fine-tuning. 2025. URL: https://arxiv.org/abs/2503.01067, arXiv:2503.01067.\u00a0\u21a9</p> </li> <li> <p>John J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554\u20132558, 1982. doi:10.1073/pnas.79.8.2554.\u00a0\u21a9</p> </li> <li> <p>Geoffrey E. Hinton and Terrence J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147\u2013169, 1985. doi:10.1207/s15516709cog0901_7.\u00a0\u21a9</p> </li> <li> <p>Claude E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27(3):379\u2013423, 1948. doi:10.1002/j.1538-7305.1948.tb01338.x.\u00a0\u21a9</p> </li> <li> <p>Edwin T. Jaynes. Information theory and statistical mechanics. Physical Review, 106(4):620\u2013630, 1957. doi:10.1103/PhysRev.106.620.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025/10/15/weighted-product-of-experts.html","title":"Test-Time Steering for Lossless Text Compression via Weighted Product of Experts","text":"<p>When I was a child, I always wondered: if I use a compressor to compress a file over and over again, will the file get smaller and smaller until it vanishes? Of course, the answer is no. If we compress the compressed data with the same compressor again, I will get a file with exactly the same size.</p> <p>Today I understand this is because of the fundamental limits of lossless compression established by information theory. But what about using multiple compressors together? If we combine multiple compressors simultaneously, can each compressor reduce part of the data's redundancy? And how can we design such a method to combine different compressors?</p> <p>This is the question that our work Test-Time Steering for Lossless Text Compression via Weighted Product of Experts <sup>1</sup> aims to answer. Compared to the EMNLP paper, this blog post focuses more on the intuition behind our method, presenting it in a way that is easier to understand.</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Test-Time Steering for Lossless Text Compression via Weighted Product of Experts<ul> <li>Table of Contents</li> <li>Background<ul> <li>LLMs can serve as powerful lossless compressors</li> <li>How to convert an auto-regressive model into a lossless compressor?</li> </ul> </li> <li>Method: Combine multiple categorical distributions via Weighted Product of Experts<ul> <li>\u2b50\ufe0f Weighted Product of Experts<ul> <li>Proof of Proposition</li> </ul> </li> <li>\u2b50\ufe0f The intuition behind the mathematical formulation<ul> <li>\u2b50\ufe0f How can we improve the ensemble wPoE model's performance from the perspective of the first term?</li> <li>\u2b50\ufe0f How can we improve the ensemble wPoE model's performance from the perspective of the second term?</li> <li>\u2b50\ufe0f We need to make the best trade-off between the first term and the second term on the target data</li> </ul> </li> </ul> </li> <li>Two Experts<ul> <li>\u2b50\ufe0f Even Combining Simple Statistical Methods Can Help LLMs Compress Better</li> <li>Experiment Results<ul> <li>Experiments on pretrained vanilla Transformers</li> <li>Experiments on GPT-2</li> <li>Experiments on LLaMA 3</li> </ul> </li> </ul> </li> <li>Multiple Experts<ul> <li>Experiment Results</li> </ul> </li> <li>Possible Applications in Other Domains</li> <li>Citation</li> </ul> </li> </ul>"},{"location":"2025/10/15/weighted-product-of-experts.html#background","title":"Background","text":""},{"location":"2025/10/15/weighted-product-of-experts.html#llms-can-serve-as-powerful-lossless-compressors","title":"LLMs can serve as powerful lossless compressors","text":"<p>The statement \"Generation is equivalent to Compression\" has been widely spread in the Machine Learning community. The relationship between these two has been established for a very long time in Information Theory:</p> <p>The target of lossless compression is minimizing cross entropy:</p> <p>If we use a distribution \\(p_{\\theta}\\) to compress data sampled from a true distribution \\(p_{data}\\), one can create a lossless compressor with an expected codelength of \\(L\\), which satisfies the following bounds: </p> \\[ H(p_{data}, p_{\\theta}) \\leq L \\leq H(p_{data}, p_{\\theta}) + 2 \\] <p>where \\(H(p_{data}, p_{\\theta})\\) is the cross entropy between the true distribution and the model distribution.</p> <p> The target of LLMs' pre-training is also minimizing cross entropy:</p> <p>It is also well-known that the training objective of large language models (LLMs) pre-training is to minimize the cross entropy between the dataset and the distribution encoded by the LLM. Therefore, the training objective of LLMs' pre-training is exactly the same as the objective of building a good lossless compressor.</p> <p>In the work Language Modelling is Compression<sup>2</sup>, it has been shown that LLMs can achieve very good compression ratios on text data, better than traditional compressors like <code>zip</code> and <code>gzip</code>. This is because LLMs can capture the complex dependencies in natural language data, which traditional compressors cannot.</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#how-to-convert-an-auto-regressive-model-into-a-lossless-compressor","title":"How to convert an auto-regressive model into a lossless compressor?","text":"<p>The method of using an existing distribution to compress data from another distribution is called source coding. The most well-known source coding algorithm is Huffman coding. </p> <p>However, with respect to computational efficiency, Arithmetic Coding is better suited for auto-regressive models like LLMs. Since Arithmetic Coding also compresses sequential data by encoding each token one by one, it can be easily combined with auto-regressive models.</p> <p>Here is an example of using Arithmetic Coding to compress a text sequence with a simple auto-regressive model:</p> <p></p>"},{"location":"2025/10/15/weighted-product-of-experts.html#method-combine-multiple-categorical-distributions-via-weighted-product-of-experts","title":"Method: Combine multiple categorical distributions via Weighted Product of Experts","text":"<p>Even before the era of LLMs, people have been trying to use neural networks like Transformers to train and build better compressors. However, the generalization ability of these models is limited by the scale of training data. Meanwhile, universal compressors like <code>gzip</code> can work well on a wide range of data, even data they have never seen before.</p> <p>Thus, it is natural to think that if we can combine universal compressors with neural-based compressors, we can achieve better compression ratios on a wide range of data. This can help neural-based compressors generalize better to potentially unseen data. In our experiments, we can see that combining with universal compressors is not only helpful for Transformers trained on small datasets, but also helpful for large LLMs like GPT-2 and LLaMA 3. Meanwhile, only very small computational overhead is introduced.</p> <p>Note on terminology: Throughout this blog post, we use several terms interchangeably</p> <ul> <li>\"Expert\" and \"model\" both refer to the individual discrete probability distributions that we combine.</li> <li>\"categorical distribution\" and \"probability distribution\" both refer to the distribution over possible next tokens that each model outputs.</li> </ul>"},{"location":"2025/10/15/weighted-product-of-experts.html#weighted-product-of-experts","title":"\u2b50\ufe0f Weighted Product of Experts","text":"<p>We propose a framework called Weighted Product of Experts (wPoE) to combine multiple distributions (also called experts or models) together so that we can guarantee the ensemble model is always no worse than the best individual expert with respect to the cross entropy with the real data distribution. </p> <p>The idea is to use a weighted product of the probability distributions of multiple experts (compressors) to form a new distribution. Here is how we define the distribution of wPoE:</p> \\[ p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}}(X_n|  X_{&lt;n}) =  \\frac{1}{Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},n)}\\displaystyle\\prod_{k = 1}^K p_{\\theta_k}(X_n  |  X_{&lt;n})^{\\alpha_k}, \\] <p>where:</p> <ul> <li>the weights are \\(\\boldsymbol{\\alpha} = \\{\\alpha_1,...,\\alpha_K\\}\\), \\(\\alpha_k \\in[0,1]\\), \\(\\displaystyle\\sum_{k = 1}^K \\alpha_k = 1\\), </li> <li>the parameters of experts are \\(\\boldsymbol{\\theta} = \\{\\theta_1,...,\\theta_K\\}\\),</li> <li>the normalization constant is \\(Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}, n) = \\displaystyle\\sum_{a \\in \\mathcal{A}}\\prod_{k = 1}^K p_{\\theta_k}(X_n = a |  X_{&lt;n})^{\\alpha_k}\\).</li> </ul> <p>And we can propose the following proposition:</p> \\[ \\displaystyle\\inf_{\\boldsymbol{\\alpha}}H(p_{\\text{data}},p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}} ) \\leq \\displaystyle\\min_{k \\in \\{1,...K\\}} H(p_{\\text{data}}, p_{\\theta_k}), \\] <p>This means we can always find a set of weights \\(\\boldsymbol{\\alpha}\\) such that the cross entropy between the data distribution and the wPoE distribution is no worse than the best individual expert.</p> <p>Although Geoffrey Hinton proposed Product of Experts <sup>3</sup> a long time ago, and there are also variants like Generalized Product of Experts<sup>4</sup>, these methods usually train the experts jointly. In contrast, we operate under a setting where each expert's distribution is fixed and cannot be changed (i.e., pretrained models).</p> Proposition Proof"},{"location":"2025/10/15/weighted-product-of-experts.html#proof-of-proposition","title":"Proof of Proposition","text":"<p>Let \\(p_{\\theta_1},p_{\\theta_2},...,p_{\\theta_K}\\) be \\(K\\) autoregressive models used to compress a sequence \\(x_{&lt;n+1} = \\{x_1, x_2, \\dots, x_n\\}\\), where \\(X_{&lt;n} \\sim p_{\\text{data}}\\). Each \\(x_i\\) takes values from the dictionary \\(\\mathcal{A} = \\{ a_1, \\dots, a_D \\}\\). For an autoregressive model \\(p_{\\theta_k}\\), the following equation reveals the relationship between the joint distribution of \\(X_{&lt;n}\\) and the conditional distribution of \\(X_n\\):</p> \\[ p_{\\theta_k}(X_{&lt;n+1}) = \\displaystyle\\prod_{i = 1}^{n} p_{\\theta_k}(X_i \\mid X_{&lt;i}). \\] <p>Therefore, the cross entropy between \\(p_{\\text{data}}\\) and a certain model \\(p_{\\theta_k}\\) can be expanded as follows:</p> \\[ H(p_{\\text{data}},p_{\\theta_k}) = \\underset{p_{\\text{data}}}{\\mathbb{E}} \\sum_{i = 1}^{n} -\\log p_{\\theta_k}(X_i \\mid X_{&lt;i}). \\] <p>Our weighted product of experts (wPoE) model is given by:</p> \\[ p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}}(X_n\\mid  X_{&lt;n}) \\;=\\;  \\frac{1}{Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},n)} \\displaystyle\\prod_{k = 1}^K p_{\\theta_k}(X_n  \\mid  X_{&lt;n})^{\\alpha_k}, \\] <p>where the weights are \\(\\boldsymbol{\\alpha} = \\{\\alpha_1,...,\\alpha_K\\}\\), \\(\\alpha_k \\in[0,1]\\), \\(\\sum_{k = 1}^K \\alpha_k = 1\\), the parameters of experts are \\(\\boldsymbol{\\theta} = \\{\\theta_1,...,\\theta_K\\}\\), and the normalization constant is</p> \\[ Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}, n) \\;=\\; \\sum_{a \\in \\mathcal{A}}\\prod_{k = 1}^K p_{\\theta_k}(X_n = a \\mid  X_{&lt;n})^{\\alpha_k}. \\] <p>Here we can derive:</p> \\[ \\begin{aligned} H(p_{\\text{data}}, p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}})  &amp;=  \\sum_{k = 1}^K \\alpha_k H(p_{\\text{data}},p_{\\theta_k}) +\\underset{p_{\\text{data}}}{\\mathbb{E}} \\displaystyle\\sum_{i=1}^{n} \\log \\left[Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i)\\right]. \\end{aligned} \\] <p>To complete the proof, we introduce the following technical lemma for bounding \\(Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i)\\).</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#lemma-1","title":"Lemma 1","text":"<p>Let \\(p^{(k)} = \\bigl(p^{(k)}_1, \\ldots, p^{(k)}_D\\bigr)\\) for \\(k=1,\\dots,K\\) be \\(K\\) categorical distributions, so \\(\\sum_{j=1}^D p^{(k)}_j = 1\\) for each \\(k\\). Let \\(\\alpha_1,\\dots,\\alpha_K \\ge 0\\) satisfy \\(\\sum_{k=1}^K \\alpha_k = 1.\\) Then</p> \\[ \\sum_{j=1}^D  \\prod_{k=1}^K \\bigl(p^{(k)}_j\\bigr)^{\\alpha_k} \\;\\;\\le\\;\\; 1, \\] <p>with equality if and only if \\(p^{(1)} = p^{(2)} = \\cdots = p^{(K)}\\) or exactly one \\(\\alpha_k=1\\) and the rest are zero.</p> <p>From <code>Cauchy\u2013Schwarz inequality</code>, it can be concluded that:</p> \\[ Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i) \\leq  1, \\quad \\forall \\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i. \\] <p>Equality holds if and only if each distribution \\(p_{\\theta_k}(X_i \\mid X_{&lt;i})\\) is the same, or \\(\\alpha_k = 1\\) and others are 0. Thus we can conclude that:</p> \\[ \\begin{aligned} \\inf_{\\boldsymbol{\\alpha}}H(p_{\\text{data}},p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}} ) &amp;\\leq \\min_{k \\in \\{1,\\dots,K\\}} H(p_{\\text{data}}, p_{\\theta_k}) + \\underset{p_{\\text{data}}}{\\mathbb{E}} \\displaystyle\\sum_{i=1}^{n} \\log \\left[Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i)\\right] \\\\ \\inf_{\\boldsymbol{\\alpha}}H(p_{\\text{data}},p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}} ) &amp;\\leq \\min_{k \\in \\{1,\\dots,K\\}} H(p_{\\text{data}}, p_{\\theta_k}). \\end{aligned} \\]"},{"location":"2025/10/15/weighted-product-of-experts.html#the-intuition-behind-the-mathematical-formulation","title":"\u2b50\ufe0f The intuition behind the mathematical formulation","text":"<p>In the proof, we can see that the cross entropy of the ensemble model can be reformulated into the following form, and this form provides intuition for why our experiments work. </p> \\[ \\begin{aligned} H(p_{\\text{data}}, p_{\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}})  &amp;=  \\sum_{k = 1}^K \\alpha_k H(p_{\\text{data}},p_{\\theta_k}) +\\underset{p_{\\text{data}}}{\\mathbb{E}} \\displaystyle\\sum_{i=1}^{n} \\log \\left[Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha},i)\\right]. \\end{aligned}, \\] <p>where:</p> \\[ Z(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}, n) \\;=\\; \\sum_{a \\in \\mathcal{A}}\\prod_{k = 1}^K p_{\\theta_k}(X_n = a \\mid  X_{&lt;n})^{\\alpha_k}. \\] <p></p>"},{"location":"2025/10/15/weighted-product-of-experts.html#how-can-we-improve-the-ensemble-wpoe-models-performance-from-the-perspective-of-the-first-term","title":"\u2b50\ufe0f How can we improve the ensemble wPoE model's performance from the perspective of the first term?","text":"<p>The first term of this formulation is the weighted average of the cross entropy between the data distribution and each expert (model), where the weights are the \\(\\alpha\\) values we want to learn or optimize using a small amount of data.</p> <p>Conclusion:</p> <p>Therefore, we can easily see that:</p> <ol> <li>The better the experts used in wPoE, the better the ensembled model is likely to be.</li> <li>The larger the weight (i.e., the corresponding \\(\\alpha\\) value) we assign to the best experts we have, the better the ensembled model is likely to be.</li> </ol> <p></p>"},{"location":"2025/10/15/weighted-product-of-experts.html#how-can-we-improve-the-ensemble-wpoe-models-performance-from-the-perspective-of-the-second-term","title":"\u2b50\ufe0f How can we improve the ensemble wPoE model's performance from the perspective of the second term?","text":"<p>The second term is the expectation of the log partition function under the data distribution. The value of this term is always smaller than or equal to <code>0</code>.</p> <p>If we look into the details of the second term, we will find that it has the following excellent properties:</p> <ol> <li> <p>This term is always smaller than or equal to <code>0</code> no matter what the data distribution is, since we didn't make any assumptions about the data distribution in the proof.</p> </li> <li> <p>This term equals zero if and only if: \\(p_{\\theta_1} = p_{\\theta_2} = \\cdots = p_{\\theta_K}\\) or exactly one \\(\\alpha_k=1\\) and the rest are <code>0</code>, as proved by the Cauchy\u2013Schwarz Inequality.</p> </li> <li> <p>The more diverse the experts are, the smaller (more negative) this term is.</p> </li> </ol> Click to expand the explanation <ol> <li>       For two-expert cases, this term behaves like a distance between two distributions. It becomes <code>0</code> only when the distributions are identical. The more diverse the distributions are, the smaller this term becomes, which helps reduce cross entropy and improves the wPoE ensemble on compression tasks.     </li> <li>For K-expert cases, treat the ensemble of the first K-1 experts as a single model. The term then captures the distance between that ensemble and the Kth expert we are about to combine.</li> <li>       This distance measures diversity differently from the usual KL divergence:       <ol> <li>It is a true distance: symmetric, so swapping the order of the two distributions does not change its value.</li> <li>The family of distances is controlled by the weights \u03b1<sub>k</sub>; when one weight is 1 and the others are 0, the distance collapses to 0.</li> <li>With a fixed set of experts, the distance is convex in the weights. Starting from \u03b1<sub>k</sub> = 1 and gradually adding more experts shrinks this second term, and the optimization is straightforward because of convexity.</li> </ol> </li> </ol> <p>Conclusion:</p> <p>Therefore, we can see that:</p> <ol> <li>If the experts are diverse (as defined from the perspective of the second term), the wPoE ensembled model is likely to have better performance.</li> <li>If the experts are truly diverse, using non-sparse \\(\\alpha\\) weights will provide better performance.</li> </ol> <p></p>"},{"location":"2025/10/15/weighted-product-of-experts.html#we-need-to-make-the-best-trade-off-between-the-first-term-and-the-second-term-on-the-target-data","title":"\u2b50\ufe0f We need to make the best trade-off between the first term and the second term on the target data","text":"<p>It is easy to see that we should try to use experts that are both high-quality and diverse when choosing which experts to combine. Thus, the best choice is to combine different models with good performance that are also diverse (e.g., trained on different data).</p> <p>Diversity is crucial in wPoE:</p> <p>Sometimes these two objectives conflict with each other. In this case, we only need to use a very small amount of data to find the optimal trade-off. In our experiments, we observe that:</p> <ol> <li> <p>When we combine another expert with good performance but not very different from the current one, the benefit that wPoE brings is quite small. (For example, when we combine two models of different sizes but trained on the same dataset.)</p> </li> <li> <p>When we combine another expert with even mediocre performance but very different from the current one, the benefit that wPoE brings is still significant and stable across various datasets and model combinations.</p> </li> </ol>"},{"location":"2025/10/15/weighted-product-of-experts.html#two-experts","title":"Two Experts","text":""},{"location":"2025/10/15/weighted-product-of-experts.html#even-combining-simple-statistical-methods-can-help-llms-compress-better","title":"\u2b50\ufe0f Even Combining Simple Statistical Methods Can Help LLMs Compress Better","text":"<p>To make LLMs perform better on potentially unseen data, we combine LLMs with simple statistical methods like Naive Bayes with Laplace smoothing, using wPoE:</p> <p>The distribution of Naive Bayes with Laplace smoothing is defined as:</p> \\[ q(X_n = a \\mid X_{&lt;n}) := \\frac{\\sum_{k=1}^{n-1} \\mathbb{I}(X_k = a) + 1}{n - 1 + D}, \\] <p>where \\(\\mathbb{I}(\\cdot)\\) denotes the indicator function and \\(D\\) is the vocabulary size. </p> <p>We then combine the Naive Bayes with Laplace smoothing \\(q\\) with a pretrained language model \\(p_{\\theta}\\) using the weighted product of experts as follows:</p> \\[ \\pi_{\\alpha}\\!(X_n \\vert X_{&lt;n}) \\!= \\!     \\frac{q(X_n \\vert X_{&lt;n})^{\\alpha} p_{\\theta}(X_n \\vert X_{&lt;n})^{1 - \\alpha}}     {Z(\\theta, \\alpha, n)}, \\] <p>where \\(\\alpha\\) is a scalar since we have only two experts. Moreover, since we do not need to fine-tune the pretrained model \\(p_{\\theta}\\) (i.e., \\(\\theta\\) is frozen), we omit the dependency on \\(\\theta\\) in the wPoE model \\(\\pi\\).</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#experiment-results","title":"Experiment Results","text":"<p>As mentioned in our paper, this combination helps various pretrained models achieve better compression rates across all datasets we collected from different sources.</p> <p>It is reasonable that as we use larger and larger models, the improvement brought by Naive Bayes becomes smaller, since larger models can already generalize better to potentially unseen data through large-scale training on vast amounts of data.</p> <p>However, what is not obvious is that even for very large LLMs like LLaMA 3-8B, the combination with Naive Bayes can still bring non-trivial improvements across various datasets. Considering how small the computational overhead of Naive Bayes is, and how weak Naive Bayes performs when used alone to compress data, this result is remarkable.</p> <p> This indicates that regardless of the performance of each individual model, the ensemble model obtained by wPoE can still benefit from the diversity of different models. </p> <p> This aligns with our intuition that diversity is key to improving the performance of wPoE.</p> <p>Note</p> <p>All experiments are conducted to evaluate the compression rates on five datasets (lower is better).</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#experiments-on-pretrained-vanilla-transformers","title":"Experiments on pretrained vanilla Transformers","text":"Expand to see the full table Tokenizer Compressor math code shakespeare enwik8* enwik9* Byte Level gzip 43.59% 36.72% 52.80% 49.14% 48.07% LZMA2 45.35% 38.61% 56.86% 51.33% 49.98% Naive Bayes 68.90% 64.65% 64.57% 66.03% 67.14% Transformer 200K 56.25% 65.67% 44.04% 31.59% 30.74% Transformer 200K + Ours 50.95% 53.94% 42.12% 31.58% 30.71% Transformer 800K 47.41% 62.13% 40.53% 25.97% 25.52% Transformer 800K + Ours 44.34% 49.68% 38.79% 25.94% 25.45% Transformer 3.2M 34.15% 41.02% 32.02% 18.53% 17.66% Transformer 3.2M + Ours 32.04% 36.61% 31.29% 18.52% 17.65%"},{"location":"2025/10/15/weighted-product-of-experts.html#experiments-on-gpt-2","title":"Experiments on GPT-2","text":"Expand to see the full table Tokenizer Compressor math code shakespeare enwik8* enwik9* BPE (GPT-2) Naive Bayes 66.41% 59.30% 49.74% 48.85% 53.43% GPT-2 17.68% 14.17% 23.44% 16.48% 16.73% GPT-2 + Ours 17.55% 14.16% 23.11% 16.42% 16.65%"},{"location":"2025/10/15/weighted-product-of-experts.html#experiments-on-llama-3","title":"Experiments on LLaMA 3","text":"Expand to see the full table Tokenizer Compressor math code shakespeare enwik8* enwik9* BPE (LLaMA 3) Naive Bayes 68.70% 47.54% 51.35% 48.87% 51.93% LLaMA 3.2-1B 8.54% 6.66% 16.51% 10.22% 10.05% LLaMA 3.2-1B + Ours 8.48% 6.64% 16.42% 10.16% 9.98% LLaMA 3.2-3B 7.56% 5.99% 13.97% 9.16% 8.93% LLaMA 3.2-3B + Ours 7.50% 5.95% 13.88% 9.09% 8.86% LLaMA 3-8B 6.90% 5.61% 4.74% 8.18% 8.10% LLaMA 3-8B + Ours 6.84% 5.57% 4.73% 8.12% 8.04%"},{"location":"2025/10/15/weighted-product-of-experts.html#multiple-experts","title":"Multiple Experts","text":"<p>Beyond combining simple statistical methods with LLMs, we can also combine multiple pretrained LLMs together to further improve compression rates.</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#experiment-results_1","title":"Experiment Results","text":"<p>The three decoder-only transformers we used are all trained on the same dataset (i.e., enwik8) but with different model sizes (i.e., 200k, 800k, and 3.2M parameters respectively). We combine them together using wPoE and evaluate the compression rates on three new datasets (i.e., math, code, and shakespeare).</p> <p>The results show that even if transformer 200k and transformer 800k are much more powerful than Naive Bayes when used alone, the improvement brought by combining Naive Bayes with LLMs is still significant. This indicates that diversity is really important to wPoE, some times even more important than the performance of each expert.</p> Expand to see the full table Compressor math code shakespeare 1 expert 34.15% 41.02% 32.02% 2 experts wPoE 33.63% 40.59% 31.99% 3 experts wPoE 33.62% 40.46% 31.97% 4 experts wPoE 31.99% 36.49% 31.35%"},{"location":"2025/10/15/weighted-product-of-experts.html#possible-applications-in-other-domains","title":"Possible Applications in Other Domains","text":"<p>To be done.</p>"},{"location":"2025/10/15/weighted-product-of-experts.html#citation","title":"Citation","text":"<p>If you find our work useful in your research, please consider citing:</p> <pre><code>@inproceedings{zhang-etal-2025-test,\n    title = \"Test-Time Steering for Lossless Text Compression via Weighted Product of Experts\",\n    author = \"Zhang, Qihang  and\n      Li, Muchen  and\n      Wang, Ziao  and\n      Liao, Renjie  and\n      Wang, Lele\",\n    editor = \"Christodoulopoulos, Christos  and\n      Chakraborty, Tanmoy  and\n      Rose, Carolyn  and\n      Peng, Violet\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2025\",\n    month = nov,\n    year = \"2025\",\n    address = \"Suzhou, China\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.findings-emnlp.110/\",\n    pages = \"2076--2088\",\n    ISBN = \"979-8-89176-335-7\",\n    abstract = \"Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively.Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE).At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as the best individual model.Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.\"\n}\n</code></pre> <p>References</p> <ol> <li> <p>Qihang Zhang, Muchen Li, Ziao Wang, Renjie Liao, and Lele Wang. Test-time steering for lossless text compression via weighted product of experts. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, 2076\u20132088. Suzhou, China, November 2025. Association for Computational Linguistics. URL: https://aclanthology.org/2025.findings-emnlp.110/.\u00a0\u21a9</p> </li> <li> <p>Gr\u00e9goire Del\u00e9tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. 2024. URL: https://arxiv.org/abs/2309.10668, arXiv:2309.10668.\u00a0\u21a9</p> </li> <li> <p>Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002.\u00a0\u21a9</p> </li> <li> <p>Yanshuai Cao and David J. Fleet. Generalized product of experts for automatic and principled fusion of gaussian process predictions. CoRR, 2014. URL: http://arxiv.org/abs/1410.7827, arXiv:1410.7827.\u00a0\u21a9</p> </li> </ol>"},{"location":"archive/2025.html","title":"2025","text":""},{"location":"category/qihangs-research.html","title":"Qihang's Research","text":""},{"location":"category/reinforcement-learning.html","title":"Reinforcement Learning","text":""},{"location":"category/information-theory.html","title":"Information Theory","text":""},{"location":"category/statistical-mechanics.html","title":"Statistical Mechanics","text":""},{"location":"category/energy-based-models.html","title":"Energy-Based Models","text":""}]}